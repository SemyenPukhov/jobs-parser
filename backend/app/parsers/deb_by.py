import asyncio
from bs4 import BeautifulSoup, ResultSet
from app.models import Job
from sqlmodel import select, Session
from datetime import datetime, timedelta
from app.utils.browser import fetch_html_async
from typing import Any, Dict, List
from app.logger import logger
from app.utils.slack import send_slack_message
from functools import lru_cache
import time
import uuid
from playwright.async_api import async_playwright,  TimeoutError as PlaywrightTimeoutError


# –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
# sem = asyncio.Semaphore(3)

# # –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞ 1 —á–∞—Å
# CACHE_TTL = timedelta(hours=1)
# parsed_urls_cache: Dict[str, tuple[datetime, Dict]] = {}


URLS = [
    "https://jobs.devby.io/?filter[specialization_title]=Front-end/JS&filter[job_types][]=remote_job",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=angular",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=node.js",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=javascript",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=typescript",

]
SOURCE = "jobs.devby.io"
base_url = "https://jobs.devby.io"


def remove_duplicate_jobs(jobs_info):
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ job_link"""
    unique_jobs = []
    seen_urls = set()

    for result in jobs_info:
        if isinstance(result, Exception) or result is None:
            continue

        for job in result:
            job_url = job.get('job_link', '')
            if job_url and job_url not in seen_urls:
                seen_urls.add(job_url)
                unique_jobs.append(job)

    return unique_jobs


async def get_job_detail(job: Dict[str, Any], browser):
    page_html = await fetch_html_async(job["job_link"], browser)
    soup = BeautifulSoup(page_html, "html.parser")
    job_description_div = soup.find("div", class_="vacancy__text")
    job_description = job_description_div.get_text()
    company_link_tag = soup.find("a", string="–ü—Ä–æ—Ñ–∏–ª—å –∫–æ–º–ø–∞–Ω–∏–∏")
    company_link = company_link_tag["href"]
    job["company_link"] = company_link
    job["job_description"] = job_description
    print(31324123412, job_description, company_link)

    return job


async def get_jobs_details_from_page(url: str, browser):
    jobs = []
    page_html = await fetch_html_async(url, browser)
    soup = BeautifulSoup(page_html, "html.parser")
    jobs_divs = soup.find_all('div', class_='vacancies-list-item')
    if len(jobs_divs) == 1:
        logger.info('üìä –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è', url)
        return None
    for job_div in jobs_divs[:-1]:
        job_link_tag = job_div.find("a")
        href = job_link_tag["href"]
        job_title = job_link_tag.get_text()
        company_div = job_div.find(
            'div', class_='vacancies-list-item__company')
        if not company_div:
            continue
        text_parts = [t for t in company_div.contents if t.name != 'span']
        company_title = ''.join(t.strip()
                                for t in text_parts if isinstance(t, str))
        job_info = {
            "title": job_title,
            "job_link": f"{base_url}/{href}",
            "company_title": company_title
        }

        jobs.append(job_info)

    return jobs


async def _scrape_devby_jobs(session: Session):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    start_time = time.time()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–∞
    stats = {
        "total_found": 0,
        "successfully_parsed": 0,
        "added_to_db": 0,
        "duplicates_skipped": 0
    }

    browser = None
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)

            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            jobs_info = await asyncio.gather(*[
                get_jobs_details_from_page(url, browser) for url in URLS
            ])

            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –ü–û–°–õ–ï –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
            await browser.close()
            browser = None  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–∫—Ä—ã—Ç—ã–π

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ scrape_devby_jobs: {e}")
        if browser:
            try:
                await browser.close()
            except:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏
        return []


# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Å –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫:
async def scrape_devby_jobs(session: Session):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    start_time = time.time()
    all_jobs = []

    stats = {
        "total_found": 0,
        "successfully_parsed": 0,
        "added_to_db": 0,
        "duplicates_skipped": 0
    }

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            jobs_info = await asyncio.gather(*[
                get_jobs_details_from_page(url, browser) for url in URLS
            ], return_exceptions=True)  # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–µ

            unique_jobs = remove_duplicate_jobs(jobs_info)
            stats["total_found"] = len(unique_jobs)

            jobs_details = await asyncio.gather(*[
                get_job_detail(j, browser) for j in unique_jobs
            ], return_exceptions=True)  # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–µ

            stats["successfully_parsed"] = len(jobs_details)

            for parsed_job in jobs_details:
                existing = session.exec(
                    select(Job).where(Job.url == parsed_job["job_link"])).first()
                if not existing:
                    job = Job(
                        title=parsed_job["title"],
                        url=parsed_job["job_link"],
                        description=parsed_job["job_description"],
                        source=SOURCE,
                        parsed_at=datetime.utcnow(),
                        company_url=parsed_job["company_link"],
                        company=parsed_job["company_title"]
                    )
                    session.add(job)
                    stats["added_to_db"] += 1
                    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {parsed_job['title']}")
                else:
                    stats["duplicates_skipped"] += 1
                    logger.info(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {existing.title}")
            session.commit()

            end_time = time.time()
            duration = end_time - start_time

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç –≤ Slack
            report = (
                f"üìä *–°–≤–æ–¥–∫–∞ –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É* {SOURCE}:\n"
                f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å: {stats['total_found']}\n"
                f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–∏–ª–∏: {stats['successfully_parsed']}\n"
                f"–î–æ–±–∞–≤–∏–ª–∏ –≤ –ë–î: {stats['added_to_db']}\n"
                f"–ü—Ä–æ–ø—É—Å—Ç–∏–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_skipped']}\n"
                f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫—É–Ω–¥"
            )
            await send_slack_message(report)

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ scrape_devby_jobs: {e}")
            return []
        finally:
            # –ë—Ä–∞—É–∑–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–∫—Ä–æ–µ—Ç—Å—è –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ async with
            pass
