import asyncio
from bs4 import BeautifulSoup, ResultSet
from app.models import Job
from sqlmodel import select, Session
from datetime import datetime, timedelta
from app.utils.browser import fetch_html_browser, get_browser_page, fetch_html_async
from typing import Any, Dict, List, Optional
from app.logger import logger
from app.utils.slack import send_slack_message
from functools import lru_cache
from playwright.async_api import async_playwright,  TimeoutError as PlaywrightTimeoutError

import time

sem = asyncio.Semaphore(10)

URLS = [
    "https://www.vseti.app/jobs?jobstype=%D0%A0%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0&level=Middle%2CSenior&location=%D0%94%D1%80%D1%83%D0%B3%D0%BE%D0%B5%2C%D0%97%D0%B0+%D1%80%D1%83%D0%B1%D0%B5%D0%B6%D0%BE%D0%BC&format=%D0%A3%D0%B4%D0%B0%D0%BB%D1%91%D0%BD%D0%BD%D0%BE"
]
SOURCE = "vseti.app"


async def process_page_throttled(job: Dict[str, str], browser):
    async with sem:
        try:
            return await get_job_details(job, browser)
        except Exception as e:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏ {job.get('title', 'Unknown')}: {str(e)}")
            return None


async def get_full_jobs_page(url: str):
    max_pages = 10
    page_num = 0

    try:
        async with get_browser_page(url) as page:
            while page_num < max_pages:
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                next_page_link = page.locator('a[aria-label="Next Page"]')

                if not await next_page_link.is_visible():
                    logger.info(
                        f"üìä –ö–Ω–æ–ø–∫–∏ 'Next Page' –±–æ–ª—å—à–µ –Ω–µ—Ç. –û—Ç–¥–∞—é –∫–æ–Ω—Ç–µ–Ω—Ç. –ü—Ä–æ–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {page_num}")
                    break

                logger.info(
                    f"üìä –ù–∞—à–µ–ª –∫–Ω–æ–ø–∫—É 'Next Page'. –ñ–º—É (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1})")
                await next_page_link.click()

                try:
                    await page.wait_for_load_state("networkidle", timeout=10000)
                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ networkidle: {e}. –ñ–¥—É –µ—â—ë 5 —Å–µ–∫—É–Ω–¥...")
                    await page.wait_for_timeout(5000)

                page_num += 1
            return await page.content()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {str(e)}")
        return None


async def get_job_details(job: Dict[str, str], browser):
    try:
        html = await fetch_html_async(job["href"], browser)
        if not html:
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π HTML –¥–ª—è {job['href']}")
            return None

        soup = BeautifulSoup(html, "html.parser")
        description_div = soup.find('div', class_="content_vacancy_div")
        job_description = description_div.get_text(
        ) if description_div is not None else "–ù–µ –Ω–∞–π–¥–µ–Ω–æ"

        company_link_tag = soup.find("a", string="–ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ –∫–æ–º–ø–∞–Ω–∏–∏")
        company_url = company_link_tag['href'] if company_link_tag else ""

        return {
            "url": job["href"],
            "title": job["title"],
            "description": job_description,
            "company_url": company_url,
            "company": job["company"]
        }
    except Exception as e:
        logger.error(
            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–µ—Ç–∞–ª–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏ {job.get('href', 'Unknown')}: {str(e)}")
        return None


async def get_job_links_from_page(html: str) -> List[Dict[str, str]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏"""
    if not html:
        logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π HTML –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏")
        return []

    try:
        soup = BeautifulSoup(html, "html.parser")
        links = soup.find_all("a", class_="card-jobs")
        job_links = []

        for job_link in links:
            try:
                title_div = job_link.find('div', class_='company-titile')
                if not title_div:
                    continue

                company_p = title_div.find('p')
                if not company_p:
                    continue

                company = company_p.get_text(strip=True)
                job_title_tag = job_link.find('h1')
                if not job_title_tag:
                    continue

                job_title = job_title_tag.get_text(strip=True)
                job_href = job_link.get('href')
                if not job_href:
                    continue

                job_details = {
                    "title": job_title,
                    "company": company,
                    "href": job_href
                }
                job_links.append(job_details)
            except Exception as e:
                logger.warning(
                    f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏: {str(e)}")
                continue

        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(job_links)} –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return job_links
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏: {str(e)}")
        return []


async def scrape_vseti_app_jobs(session: Session):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    start_time = time.time()
    all_jobs = []

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–∞
    stats = {
        "total_found": 0,
        "successfully_parsed": 0,
        "added_to_db": 0,
        "duplicates_skipped": 0
    }

    try:
        # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü
        tasks = [get_full_jobs_page(url) for url in URLS]
        html_pages = await asyncio.gather(*tasks, return_exceptions=True)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        valid_html_pages = []
        for i, html in enumerate(html_pages):
            if isinstance(html, Exception):
                logger.error(
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ URL {URLS[i]}: {str(html)}")
            elif html is not None:
                valid_html_pages.append(html)
            else:
                logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π HTML –¥–ª—è URL {URLS[i]}")

        if not valid_html_pages:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            return []

        # –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏
        job_links_tasks = [get_job_links_from_page(
            html) for html in valid_html_pages]
        jobs_nested = await asyncio.gather(*job_links_tasks, return_exceptions=True)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –æ–¥–∏–Ω —Å–ø–∏—Å–æ–∫
        jobs = []
        for job_list in jobs_nested:
            if isinstance(job_list, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Å—ã–ª–æ–∫: {str(job_list)}")
            elif job_list is not None:
                jobs.extend(job_list)

        if not jobs:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏")
            return []

        stats["total_found"] = len(jobs)
        logger.info(
            f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(jobs)} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")

        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)

            all_results = await asyncio.gather(*[
                process_page_throttled(job, browser) for job in jobs
            ], return_exceptions=True)

            await browser.close()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        valid_jobs = []
        for result in all_results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏: {str(result)}")
            elif result is not None:
                valid_jobs.append(result)

        stats["successfully_parsed"] = len(valid_jobs)
        logger.info(f"üìä –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(valid_jobs)} –≤–∞–∫–∞–Ω—Å–∏–π")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        for job_info in valid_jobs:
            try:
                existing = session.exec(
                    select(Job).where(Job.url == job_info["url"])).first()
                if not existing:
                    job = Job(
                        title=job_info["title"],
                        url=job_info["url"],
                        description=job_info["description"],
                        source=SOURCE,
                        parsed_at=datetime.utcnow(),
                        company_url=job_info.get("company_url", ""),
                        company=job_info["company"]
                    )
                    session.add(job)
                    all_jobs.append(job)
                    stats["added_to_db"] += 1
                    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {job_info['title']}")
                else:
                    stats["duplicates_skipped"] += 1
                    logger.info(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {existing.title}")
            except Exception as e:
                logger.error(
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ {job_info.get('title', 'Unknown')}: {str(e)}")
                continue

        if all_jobs:
            try:
                session.commit()
                logger.info(f"‚úÖ –ö–æ–º–º–∏—Ç –≤ –ë–î —É—Å–ø–µ—à–µ–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–º–º–∏—Ç–µ –≤ –ë–î: {str(e)}")
                session.rollback()

        end_time = time.time()
        duration = end_time - start_time

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç –≤ Slack
        report = (
            f"üìä *–°–≤–æ–¥–∫–∞ –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É* {SOURCE}:\n"
            f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å: {stats['total_found']}\n"
            f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–∏–ª–∏: {stats['successfully_parsed']}\n"
            f"–î–æ–±–∞–≤–∏–ª–∏ –≤ –ë–î: {stats['added_to_db']}\n"
            f"–ü—Ä–æ–ø—É—Å—Ç–∏–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_skipped']}\n"
            f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫—É–Ω–¥"
        )
        await send_slack_message(report)

        logger.info(
            f"‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {duration:.2f} —Å–µ–∫—É–Ω–¥. –î–æ–±–∞–≤–ª–µ–Ω–æ {len(all_jobs)} –≤–∞–∫–∞–Ω—Å–∏–π")

        return all_jobs
    except Exception as e:
        error_message = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {str(e)}"
        logger.error(error_message)
        await send_slack_message(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {SOURCE}:\n{str(e)}")
        return []
