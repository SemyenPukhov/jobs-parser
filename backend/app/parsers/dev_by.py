import asyncio
from bs4 import BeautifulSoup, ResultSet
from app.models import Job
from sqlmodel import select, Session
from datetime import datetime, timedelta
from app.utils.browser import fetch_html_async
from typing import Any, Dict, List, Optional
from app.logger import logger
from app.utils.slack import send_slack_message
from functools import lru_cache
import time
import uuid
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError


# –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
MAX_CONCURRENT_TABS = 5
sem = asyncio.Semaphore(MAX_CONCURRENT_TABS)

URLS = [
    "https://jobs.devby.io/?filter[specialization_title]=Front-end/JS&filter[job_types][]=remote_job",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=angular",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=node.js",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=javascript",
    "https://jobs.devby.io/?filter[job_types][]=remote_job&filter[search]=typescript",
]

SOURCE = "jobs.devby.io"
BASE_URL = "https://jobs.devby.io"


def remove_duplicate_jobs(jobs_info: List[Any]) -> List[Dict[str, Any]]:
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ job_link"""
    unique_jobs = []
    seen_urls = set()

    for result in jobs_info:
        if isinstance(result, Exception) or result is None:
            logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            continue

        for job in result:
            job_url = job.get('job_link', '')
            if job_url and job_url not in seen_urls:
                seen_urls.add(job_url)
                unique_jobs.append(job)

    return unique_jobs


async def get_job_detail(job: Dict[str, Any], browser) -> Optional[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞—Ñ–æ—Ä–∞"""
    async with sem:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        try:
            page_html = await fetch_html_async(job["job_link"], browser)
            if not page_html:
                logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è {job['job_link']}")
                return None

            soup = BeautifulSoup(page_html, "html.parser")

            # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            job_description_div = soup.find("div", class_="vacancy__text")
            job_description = job_description_div.get_text(
                strip=True) if job_description_div else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

            # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–æ–º–ø–∞–Ω–∏—é
            company_link_tag = soup.find("a", string="–ü—Ä–æ—Ñ–∏–ª—å –∫–æ–º–ø–∞–Ω–∏–∏")
            company_link = company_link_tag.get(
                "href", "") if company_link_tag else ""

            job["company_link"] = company_link
            job["job_description"] = job_description

            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –¥–ª—è: {job['title']}")
            return job

        except Exception as e:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {job.get('title', 'Unknown')}: {e}")
            return None


async def get_jobs_details_from_page(url: str, browser) -> Optional[List[Dict[str, Any]]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞—Ñ–æ—Ä–∞"""
    async with sem:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        try:
            jobs = []
            page_html = await fetch_html_async(url, browser)

            if not page_html:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è {url}")
                return None

            soup = BeautifulSoup(page_html, "html.parser")
            jobs_divs = soup.find_all('div', class_='vacancies-list-item')

            if len(jobs_divs) <= 1:
                logger.info(f'üìä –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è {url}')
                return []

            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç (–æ–±—ã—á–Ω–æ —ç—Ç–æ –ø–∞–≥–∏–Ω–∞—Ü–∏—è)
            for job_div in jobs_divs[:-1]:
                try:
                    job_link_tag = job_div.find("a")
                    if not job_link_tag:
                        continue

                    href = job_link_tag.get("href", "")
                    job_title = job_link_tag.get_text(strip=True)

                    company_div = job_div.find(
                        'div', class_='vacancies-list-item__company')
                    if not company_div:
                        continue

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
                    text_parts = [
                        t for t in company_div.contents if t.name != 'span']
                    company_title = ''.join(
                        t.strip() for t in text_parts if isinstance(t, str)).strip()

                    if not company_title:
                        company_title = "–ö–æ–º–ø–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

                    job_info = {
                        "title": job_title,
                        "job_link": f"{BASE_URL}/{href.lstrip('/')}",
                        "company_title": company_title
                    }

                    jobs.append(job_info)

                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
                    continue

            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(jobs)} –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")
            return jobs

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None


async def scrape_devby_jobs(session: Session) -> List[Dict[str, Any]]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ —Å–µ–º–∞—Ñ–æ—Ä–æ–º"""
    start_time = time.time()
    all_jobs = []

    stats = {
        "total_found": 0,
        "successfully_parsed": 0,
        "added_to_db": 0,
        "duplicates_skipped": 0,
        "errors": 0
    }

    logger.info(
        f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {SOURCE} —Å –º–∞–∫—Å–∏–º—É–º {MAX_CONCURRENT_TABS} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –≤–∫–ª–∞–¥–∫–∞–º–∏")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )

        try:
            # –≠—Ç–∞–ø 1: –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            logger.info("üìã –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π...")
            jobs_info = await asyncio.gather(*[
                get_jobs_details_from_page(url, browser) for url in URLS
            ], return_exceptions=True)

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_jobs = remove_duplicate_jobs(jobs_info)
            stats["total_found"] = len(unique_jobs)
            logger.info(
                f"üìä –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {stats['total_found']}")

            if not unique_jobs:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return []

            # –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
            logger.info("üîç –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö...")
            jobs_details = await asyncio.gather(*[
                get_job_detail(job, browser) for job in unique_jobs
            ], return_exceptions=True)

            # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            successful_jobs = []
            for job_detail in jobs_details:
                if isinstance(job_detail, Exception):
                    stats["errors"] += 1
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: {job_detail}")
                elif job_detail is not None:
                    successful_jobs.append(job_detail)

            stats["successfully_parsed"] = len(successful_jobs)
            logger.info(
                f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {stats['successfully_parsed']}")

            # –≠—Ç–∞–ø 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            for parsed_job in successful_jobs:
                try:
                    existing = session.exec(
                        select(Job).where(Job.url == parsed_job["job_link"])
                    ).first()

                    if not existing:
                        job = Job(
                            title=parsed_job["title"],
                            url=parsed_job["job_link"],
                            description=parsed_job["job_description"],
                            source=SOURCE,
                            parsed_at=datetime.utcnow(),
                            company_url=parsed_job.get("company_link", ""),
                            company=parsed_job["company_title"]
                        )
                        session.add(job)
                        stats["added_to_db"] += 1
                        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {parsed_job['title']}")
                    else:
                        stats["duplicates_skipped"] += 1
                        logger.info(
                            f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {existing.title}")

                except Exception as e:
                    logger.error(
                        f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ {parsed_job.get('title', 'Unknown')}: {e}")
                    stats["errors"] += 1

            session.commit()
            logger.info("üíæ –ò–∑–º–µ–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ scrape_devby_jobs: {e}")
            stats["errors"] += 1
            session.rollback()
        finally:
            await browser.close()
            logger.info("üîí –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç
    end_time = time.time()
    duration = end_time - start_time

    report = (
        f"üìä *–°–≤–æ–¥–∫–∞ –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É {SOURCE}*:\n"
        f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {stats['total_found']}\n"
        f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['successfully_parsed']}\n"
        f"–î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –ë–î: {stats['added_to_db']}\n"
        f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_skipped']}\n"
        f"–û—à–∏–±–æ–∫: {stats['errors']}\n"
        f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫—É–Ω–¥\n"
        f"–ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫: {MAX_CONCURRENT_TABS}"
    )

    try:
        await send_slack_message(report)
        logger.info("üì§ –û—Ç—á–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ Slack")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç—á–µ—Ç–∞ –≤ Slack: {e}")

    logger.info(report)
    return successful_jobs


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ–º–∞—Ñ–æ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
def set_max_concurrent_tabs(max_tabs: int):
    """–ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫"""
    global sem, MAX_CONCURRENT_TABS
    MAX_CONCURRENT_TABS = max_tabs
    sem = asyncio.Semaphore(max_tabs)
    logger.info(
        f"üîß –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫: {max_tabs}")
