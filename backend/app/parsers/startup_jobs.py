import asyncio
from bs4 import BeautifulSoup, ResultSet
from app.models import Job
from sqlmodel import select, Session
from datetime import datetime, timedelta
from app.utils.browser import fetch_html_browser
from typing import Any, Dict, List
from app.logger import logger
from functools import lru_cache
import time
import uuid

# –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
sem = asyncio.Semaphore(3)

# –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞ 1 —á–∞—Å
CACHE_TTL = timedelta(hours=1)
parsed_urls_cache: Dict[str, tuple[datetime, Dict]] = {}


URLS = [
    "https://startup.jobs/?remote=true&since=24h&q=%22React%22",
    "https://startup.jobs/?remote=true&since=24h&q=%22Node%22",
    "https://startup.jobs/?remote=true&since=24h&q=%22Angular%22",
    "https://startup.jobs/?remote=true&since=24h&q=javascript",
    "https://startup.jobs/?remote=true&since=24h&q=web+developer",
    "https://startup.jobs/?remote=true&since=24h&q=python",
]
SOURCE = "startup.jobs"
base_url = "https://startup.jobs"


def get_cached_result(url: str) -> Dict | None:
    """–ü–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ None –µ—Å–ª–∏ –∫—ç—à —É—Å—Ç–∞—Ä–µ–ª"""
    if url in parsed_urls_cache:
        timestamp, result = parsed_urls_cache[url]
        if datetime.utcnow() - timestamp < CACHE_TTL:
            return result
        del parsed_urls_cache[url]
    return None


def cache_result(url: str, result: Dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
    parsed_urls_cache[url] = (datetime.utcnow(), result)


async def process_job_div_throttled(job_div):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ div —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    async with sem:
        try:
            return await process_job_div(job_div)
        except Exception as e:
            logger.error(f"Error processing job div: {str(e)}")
            return None


def find_apply_link(s: BeautifulSoup) -> str | None:
    """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–¥–∞—á—É –∑–∞—è–≤–∫–∏"""
    for a in s.find_all("a"):
        if "Apply for this job" in a.get_text(strip=True):
            return f"{base_url}/{a['href'].lstrip('/')}"
    return None


async def get_job_description(url: str) -> Dict[str, str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_result = get_cached_result(url)
    if cached_result:
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è {url}")
        return cached_result

    try:
        job_html = await fetch_html_browser(url)
        soup = BeautifulSoup(job_html, "html.parser")
        desc_div = soup.find("div", class_=["trix-content"])
        apply_url = find_apply_link(soup)

        if not desc_div:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è {url}")
            return {"description": "", "apply_url": apply_url}

        result = {
            "description": desc_div.get_text(),
            "apply_url": apply_url
        }

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        cache_result(url, result)
        return result
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è {url}: {str(e)}")
        return {"description": "", "apply_url": None}


async def process_job_div(job_div: ResultSet[Any]) -> Dict | None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ div —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π"""
    try:
        link_tag = job_div.find(
            "a", attrs={"data-mark-visited-links-target": "anchor"})
        if not link_tag:
            return None

        href = link_tag["href"].lstrip("/")
        full_job_url = f"{base_url}/{href}"

        company_link_tag = link_tag.find_next("a")
        if not company_link_tag:
            return None

        company_href = company_link_tag["href"].lstrip("/")
        company_url = f"{base_url}/{company_href}"
        company_name = company_link_tag.get_text()
        job_title = " ".join(line.strip() for line in link_tag.get_text(
        ).strip().splitlines() if line.strip())

        job_info = await get_job_description(full_job_url)

        return {
            "url": full_job_url,
            "company_url": company_url,
            "company_name": company_name,
            "title": job_title,
            "description": job_info["description"],
            "apply_url": job_info["apply_url"]
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏: {str(e)}")
        return None


async def parse_jobs_from_html(html: str) -> List[Job]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ HTML"""
    try:
        soup = BeautifulSoup(html, "html.parser")
        hits_div = soup.find("div", attrs={"data-search-target": "hits"})
        if not hits_div:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω div —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏")
            return []

        job_rows = hits_div.find_all("div", class_="isolate")
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(job_rows)} –≤–∞–∫–∞–Ω—Å–∏–π")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        parsed_jobs = await asyncio.gather(
            *[process_job_div_throttled(job) for job in job_rows],
            return_exceptions=True
        )

        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        parsed_jobs = [
            job for job in parsed_jobs if job is not None and not isinstance(job, Exception)]

        jobs = []
        for job in parsed_jobs:
            job_data = Job(
                title=job["title"],
                url=job["url"],
                source=SOURCE,
                description=job["description"],
                company=job["company_name"],
                company_url=job["company_url"],
                parsed_at=datetime.utcnow(),
                apply_url=job["apply_url"]
            )
            jobs.append(job_data)

        return jobs
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML: {str(e)}")
        return []


async def scrape_startup_jobs(session: Session):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    start_time = time.time()
    all_jobs = []

    try:
        screenshot_uuid = str(uuid.uuid4())[:8]
        # –ü–æ–ª—É—á–∞–µ–º HTML —Å–æ –≤—Å–µ—Ö URL —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        tasks = [fetch_html_browser(
            url, f"startup_jobs_{screenshot_uuid}.png") for url in URLS]
        html_results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for html in html_results:
            if isinstance(html, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ HTML: {str(html)}")
                continue

            jobs = await parse_jobs_from_html(html)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            for job in jobs:
                existing = session.exec(
                    select(Job).where(Job.url == job.url)).first()

                if not existing:
                    logger.info(f"üìä –°–æ—Ö—Ä–∞–Ω—è—é –≤ –ë–î {job.url}")
                    session.add(job)
                    all_jobs.append(job)
                else:
                    logger.info(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é –¥—É–±–ª–∏–∫–∞—Ç {job.url}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ–¥–Ω–∏–º –∫–æ–º–º–∏—Ç–æ–º
        if all_jobs:
            session.commit()

        end_time = time.time()
        logger.info(
            f"‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥. –î–æ–±–∞–≤–ª–µ–Ω–æ {len(all_jobs)} –≤–∞–∫–∞–Ω—Å–∏–π")

        return all_jobs
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {str(e)}")
        return []
